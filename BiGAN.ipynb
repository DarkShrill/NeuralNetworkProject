{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BiGAN__TEST.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Y-BEy7WW1sg"
      },
      "source": [
        "import torch\r\n",
        "import numpy as np\r\n",
        "from torchvision import datasets\r\n",
        "from torch.utils.data import DataLoader\r\n",
        "import torchvision.transforms as transforms\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "from torch import optim\r\n",
        "from torch.autograd import Variable\r\n",
        "import torchvision.utils as vutils\r\n",
        "import torchvision as torchvision\r\n",
        "!pip install barbar\r\n",
        "from barbar import Bar\r\n",
        "# !pip install cifar\r\n",
        "\r\n",
        "from google.colab import drive\r\n",
        "\r\n",
        "print(torch.__version__)\r\n",
        "\r\n",
        "# drive.mount('/content/drive')\r\n",
        "\r\n",
        "DATASET_DIR = '/content/drive/My Drive/data/cifar/'\r\n",
        "# DATASET_DIR = '/DATASET/SVHN/'\r\n",
        "# DATASET_DIR = '/DATASET/CELEB_A/'\r\n",
        "IMAGE_DIR = '/content/drive/My Drive/data/images/'\r\n",
        "\r\n",
        "DATASET = \"NULL\"\r\n",
        "\r\n",
        "def get_cifar10(args, data_dir=DATASET_DIR):\r\n",
        "    \"\"\"Returning cifar dataloder.\"\"\"\r\n",
        "    transform = transforms.Compose([transforms.Resize(32), #3x32x32 images.\r\n",
        "                                    transforms.ToTensor()])\r\n",
        "    train = datasets.CIFAR10(root=data_dir, train=True, download=True, transform=transform)\r\n",
        "    test = datasets.CIFAR10(root=data_dir, train=False, download=True, transform=transform)\r\n",
        "    train_dataloader = DataLoader(train, batch_size=args.batch_size, shuffle=True)\r\n",
        "    test_dataloader = DataLoader(test, batch_size=args.batch_size, shuffle=True)\r\n",
        "    DATASET = \"cifar10\"\r\n",
        "    return train_dataloader,test_dataloader\r\n",
        "\r\n",
        "def get_SVHN(args, data_dir=DATASET_DIR):\r\n",
        "    \"\"\"Returning cifar dataloder.\"\"\"\r\n",
        "    transform = transforms.Compose([transforms.Resize(32), #3x32x32 images.\r\n",
        "                                    transforms.ToTensor()])\r\n",
        "    train = datasets.SVHN(root=data_dir, split='train', download=True, transform=transform)\r\n",
        "    test = datasets.SVHN(root=data_dir, split='test', download=True, transform=transform)\r\n",
        "    train_dataloader = DataLoader(train, batch_size=args.batch_size, shuffle=True)\r\n",
        "    test_dataloader = DataLoader(test, batch_size=args.batch_size, shuffle=True)\r\n",
        "    DATASET = \"SVHN\"\r\n",
        "    return train_dataloader,test_dataloader\r\n",
        "\r\n",
        "def get_CELEB_A(args, data_dir=DATASET_DIR):\r\n",
        "    \"\"\"Returning cifar dataloder.\"\"\"\r\n",
        "    transform = transforms.Compose([transforms.Resize(32), #3x32x32 images.\r\n",
        "                                    transforms.ToTensor()])\r\n",
        "    train = datasets.CelebA(root=data_dir, split='train', download=True, transform=transform)\r\n",
        "    test = datasets.CelebA(root=data_dir, split='test', download=True, transform=transform)\r\n",
        "    train_dataloader = DataLoader(train, batch_size=args.batch_size, shuffle=True)\r\n",
        "    test_dataloader = DataLoader(test, batch_size=args.batch_size, shuffle=True)\r\n",
        "    return train_dataloader,test_dataloader\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RaP3xdv-YLlQ"
      },
      "source": [
        "def weights_init_normal(m):\r\n",
        "    classname = m.__class__.__name__\r\n",
        "    if classname.find(\"Conv\") != -1 and classname != 'Conv':\r\n",
        "        torch.nn.init.normal_(m.weight.data, 0.0, 1e-3)\r\n",
        "        if m.bias is not None:\r\n",
        "            m.bias.data.fill_(0)\r\n",
        "    elif classname.find(\"Linear\") != -1:\r\n",
        "        torch.nn.init.normal_(m.weight.data, 0.0, 1e-3)\r\n",
        "        if m.bias is not None:\r\n",
        "            m.bias.data.fill_(0)\r\n",
        "    elif classname.find('BatchNorm') != -1:\r\n",
        "        m.weight.data.normal_(1.0, 0.01)\r\n",
        "        if m.bias is not None:\r\n",
        "            m.bias.data.fill_(0)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1XxRaD9YRZw"
      },
      "source": [
        "class Discriminator(nn.Module):\r\n",
        "    def __init__(self, z_dim=32, wasserstein=False):\r\n",
        "        super(Discriminator, self).__init__()\r\n",
        "        self.wass = wasserstein\r\n",
        "\r\n",
        "        #Â Inference over x\r\n",
        "        self.conv1x = nn.Conv2d(3, 32, 5, stride=1, bias=False)\r\n",
        "        self.conv2x = nn.Conv2d(32, 64, 4, stride=2, bias=False)\r\n",
        "        self.bn2x = nn.BatchNorm2d(64)\r\n",
        "        self.conv3x = nn.Conv2d(64, 128, 4, stride=1, bias=False)\r\n",
        "        self.bn3x = nn.BatchNorm2d(128)\r\n",
        "        self.conv4x = nn.Conv2d(128, 256, 4, stride=2, bias=False)\r\n",
        "        self.bn4x = nn.BatchNorm2d(256)\r\n",
        "        self.conv5x = nn.Conv2d(256, 512, 4, stride=1, bias=False)\r\n",
        "        self.bn5x = nn.BatchNorm2d(512)\r\n",
        "\r\n",
        "        # Inference over z\r\n",
        "        self.conv1z = nn.Conv2d(z_dim, 512, 1, stride=1, bias=False)\r\n",
        "        self.conv2z = nn.Conv2d(512, 512, 1, stride=1, bias=False)\r\n",
        "\r\n",
        "        # Joint inference\r\n",
        "        self.conv1xz = nn.Conv2d(1024, 1024, 1, stride=1, bias=False)\r\n",
        "        self.conv2xz = nn.Conv2d(1024, 1024, 1, stride=1, bias=False)\r\n",
        "        self.conv3xz = nn.Conv2d(1024, 1, 1, stride=1, bias=False)\r\n",
        "\r\n",
        "    def inf_x(self, x):\r\n",
        "        x = F.dropout2d(F.leaky_relu(self.conv1x(x), negative_slope=0.1), 0.2)\r\n",
        "        x = F.dropout2d(F.leaky_relu(self.bn2x(self.conv2x(x)), negative_slope=0.1), 0.2)\r\n",
        "        x = F.dropout2d(F.leaky_relu(self.bn3x(self.conv3x(x)), negative_slope=0.1), 0.2)\r\n",
        "        x = F.dropout2d(F.leaky_relu(self.bn4x(self.conv4x(x)), negative_slope=0.1), 0.2)\r\n",
        "        x = F.dropout2d(F.leaky_relu(self.bn5x(self.conv5x(x)), negative_slope=0.1), 0.2)\r\n",
        "        return x\r\n",
        "\r\n",
        "    def inf_z(self, z):\r\n",
        "        z = F.dropout2d(F.leaky_relu(self.conv1z(z), negative_slope=0.1), 0.2)\r\n",
        "        z = F.dropout2d(F.leaky_relu(self.conv2z(z), negative_slope=0.1), 0.2)\r\n",
        "        return z\r\n",
        "\r\n",
        "    def inf_xz(self, xz):\r\n",
        "        xz = F.dropout(F.leaky_relu(self.conv1xz(xz), negative_slope=0.1), 0.2)\r\n",
        "        xz = F.dropout(F.leaky_relu(self.conv2xz(xz), negative_slope=0.1), 0.2)\r\n",
        "        return self.conv3xz(xz)\r\n",
        "\r\n",
        "    def forward(self, x, z):\r\n",
        "        x = self.inf_x(x)\r\n",
        "        z = self.inf_z(z)\r\n",
        "        xz = torch.cat((x,z), dim=1)\r\n",
        "        out = self.inf_xz(xz)\r\n",
        "        if self.wass:\r\n",
        "            return out\r\n",
        "        else:\r\n",
        "            return torch.sigmoid(out)\r\n",
        "\r\n",
        "\r\n",
        "class Generator(nn.Module):\r\n",
        "    def __init__(self, z_dim=32):\r\n",
        "        super(Generator, self).__init__()\r\n",
        "        self.z_dim = z_dim\r\n",
        "\r\n",
        "        self.output_bias = nn.Parameter(torch.zeros(3, 32, 32), requires_grad=True)\r\n",
        "        self.deconv1 = nn.ConvTranspose2d(z_dim, 256, 4, stride=1, bias=False)\r\n",
        "        self.bn1 = nn.BatchNorm2d(256)\r\n",
        "        self.deconv2 = nn.ConvTranspose2d(256, 128, 4, stride=2, bias=False)\r\n",
        "        self.bn2 = nn.BatchNorm2d(128)\r\n",
        "        self.deconv3 = nn.ConvTranspose2d(128, 64, 4, stride=1, bias=False)\r\n",
        "        self.bn3 = nn.BatchNorm2d(64)\r\n",
        "        self.deconv4 = nn.ConvTranspose2d(64, 32, 4, stride=2, bias=False)\r\n",
        "        self.bn4 = nn.BatchNorm2d(32)\r\n",
        "        self.deconv5 = nn.ConvTranspose2d(32, 32, 5, stride=1, bias=False)\r\n",
        "        self.bn5 = nn.BatchNorm2d(32)\r\n",
        "        self.deconv6 = nn.Conv2d(32, 3, 1, stride=1, bias=True)\r\n",
        "\r\n",
        "    def forward(self, z):\r\n",
        "        z = F.leaky_relu(self.bn1(self.deconv1(z)), negative_slope=0.1)\r\n",
        "        z = F.leaky_relu(self.bn2(self.deconv2(z)), negative_slope=0.1)\r\n",
        "        z = F.leaky_relu(self.bn3(self.deconv3(z)), negative_slope=0.1)\r\n",
        "        z = F.leaky_relu(self.bn4(self.deconv4(z)), negative_slope=0.1)\r\n",
        "        z = F.leaky_relu(self.bn5(self.deconv5(z)), negative_slope=0.1)\r\n",
        "        return torch.sigmoid(self.deconv6(z) + self.output_bias)\r\n",
        "\r\n",
        "\r\n",
        "class Encoder(nn.Module):\r\n",
        "    def __init__(self, z_dim=32):\r\n",
        "        super(Encoder, self).__init__()\r\n",
        "        self.z_dim = z_dim\r\n",
        "        self.conv1 = nn.Conv2d(3, 32, 5, stride=1, bias=False)\r\n",
        "        self.bn1 = nn.BatchNorm2d(32)\r\n",
        "        self.conv2 = nn.Conv2d(32, 64, 4, stride=2, bias=False)\r\n",
        "        self.bn2 = nn.BatchNorm2d(64)\r\n",
        "        self.conv3 = nn.Conv2d(64, 128, 4, stride=1, bias=False)\r\n",
        "        self.bn3 = nn.BatchNorm2d(128)\r\n",
        "        self.conv4 = nn.Conv2d(128, 256, 4, stride=2, bias=False)\r\n",
        "        self.bn4 = nn.BatchNorm2d(256)\r\n",
        "        self.conv5 = nn.Conv2d(256, 512, 4, stride=1, bias=False)\r\n",
        "        self.bn5 = nn.BatchNorm2d(512)\r\n",
        "        self.conv6 = nn.Conv2d(512, 512, 1, stride=1, bias=False)\r\n",
        "        self.bn6 = nn.BatchNorm2d(512)\r\n",
        "        self.bn7 = nn.Conv2d(512, z_dim*2, 1, stride=1, bias=True)\r\n",
        "\r\n",
        "    def reparameterize(self, z):\r\n",
        "        z = z.view(z.size(0), -1)\r\n",
        "        mu, log_sigma = z[:, :self.z_dim], z[:, self.z_dim:]\r\n",
        "        std = torch.exp(log_sigma)\r\n",
        "        eps = torch.randn_like(std)\r\n",
        "        return mu + eps * std\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        x = F.leaky_relu(self.bn1(self.conv1(x)), negative_slope=0.1)\r\n",
        "        x = F.leaky_relu(self.bn2(self.conv2(x)), negative_slope=0.1)\r\n",
        "        x = F.leaky_relu(self.bn3(self.conv3(x)), negative_slope=0.1)\r\n",
        "        x = F.leaky_relu(self.bn4(self.conv4(x)), negative_slope=0.1)\r\n",
        "        x = F.leaky_relu(self.bn5(self.conv5(x)), negative_slope=0.1)\r\n",
        "        x = F.leaky_relu(self.bn6(self.conv6(x)), negative_slope=0.1)\r\n",
        "        z = self.reparameterize(self.conv6(x))\r\n",
        "        return z.view(x.size(0), self.z_dim, 1, 1)\r\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_u5CsZCYUjX"
      },
      "source": [
        "!pip install tqdm\r\n",
        "from tqdm import tqdm\r\n",
        "import matplotlib.image as mpimg\r\n",
        "import os\r\n",
        "\r\n",
        "def D_loss(DG, DE, eps=1e-6):\r\n",
        "    loss = torch.log(DE + eps) + torch.log(1 - DG + eps)\r\n",
        "    return -torch.mean(loss)\r\n",
        "\r\n",
        "def EG_loss(DG, DE, eps=1e-6):\r\n",
        "  loss = torch.log(DG + eps) + torch.log(1 - DE + eps)\r\n",
        "  return -torch.mean(loss)\r\n",
        "\r\n",
        "if not os.path.exists(\"/prj_gen_images\"):\r\n",
        "    os.makedirs(\"/prj_gen_images\")\r\n",
        "\r\n",
        "class TrainerBiGAN:\r\n",
        "    def __init__(self, args, data, device):\r\n",
        "        self.args = args\r\n",
        "        self.train_loader = data\r\n",
        "        self.device = device\r\n",
        "\r\n",
        "\r\n",
        "    def getModel(self):\r\n",
        "      return self.G,self.D,self.E \r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "    def train(self):\r\n",
        "        \"\"\"Training the BiGAN\"\"\"\r\n",
        "        self.G = Generator(self.args.latent_dim).to(self.device)\r\n",
        "        self.E = Encoder(self.args.latent_dim).to(self.device)\r\n",
        "        self.D = Discriminator(self.args.latent_dim, self.args.wasserstein).to(self.device)\r\n",
        "\r\n",
        "        self.G.apply(weights_init_normal)\r\n",
        "        self.E.apply(weights_init_normal)\r\n",
        "        self.D.apply(weights_init_normal)\r\n",
        "\r\n",
        "        if self.args.wasserstein:\r\n",
        "            optimizer_ge = optim.RMSprop(list(self.G.parameters()) +\r\n",
        "                                         list(self.E.parameters()), lr=self.args.lr_rmsprop)\r\n",
        "            optimizer_d = optim.RMSprop(self.D.parameters(), lr=self.args.lr_rmsprop)\r\n",
        "        else:\r\n",
        "            optimizer_ge = optim.Adam(list(self.G.parameters()) +\r\n",
        "                                      list(self.E.parameters()), lr=self.args.lr_adam)\r\n",
        "            optimizer_d = optim.Adam(self.D.parameters(), lr=self.args.lr_adam)\r\n",
        "\r\n",
        "        fixed_z = Variable(torch.randn((16, self.args.latent_dim, 1, 1)),\r\n",
        "                           requires_grad=False).to(self.device)\r\n",
        "        # criterion = nn.BCELoss()\r\n",
        "        stored_d_loss = []\r\n",
        "        stored_ge_loss = []\r\n",
        "        for epoch in range(self.args.num_epochs+1):\r\n",
        "            ge_loss_accuracy = 0\r\n",
        "            d_loss_accuracy = 0\r\n",
        "\r\n",
        "            for i,(x,label)  in enumerate(tqdm(self.train_loader)):\r\n",
        "\r\n",
        "                ################################################################################################\r\n",
        "                #######################                                         ################################\r\n",
        "                #######################         DISCRIMINATOR TRAINING          ################################\r\n",
        "                #######################                                         ################################\r\n",
        "                ################################################################################################\r\n",
        "\r\n",
        "                # Cleaning gradient of D.\r\n",
        "                optimizer_d.zero_grad()\r\n",
        "\r\n",
        "                # Generator:\r\n",
        "                z_fake = Variable(torch.randn((x.size(0), self.args.latent_dim, 1, 1)).to(self.device),\r\n",
        "                                  requires_grad=False)\r\n",
        "                # compute G(z)\r\n",
        "                x_fake = self.G(z_fake.to(self.device))\r\n",
        "\r\n",
        "                # Encoder:\r\n",
        "                x_true = x.float().to(self.device)\r\n",
        "                # compute E(x)\r\n",
        "                z_true = self.E(x_true)\r\n",
        "\r\n",
        "                # Discriminator\r\n",
        "                # compute D(x, E(x))\r\n",
        "                out_true = self.D(x_true , z_true)#.squeeze(1).squeeze(1)\r\n",
        "                # compute D(G(z),z)\r\n",
        "                out_fake = self.D(x_fake , z_fake)#.squeeze(1).squeeze(1)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "                # compute losses\r\n",
        "                loss_d = D_loss(out_fake,out_true)\r\n",
        "                d_loss_accuracy += loss_d.item()\r\n",
        "                # loss_d = criterion(out_true, y_true) + criterion(out_fake, y_fake)\r\n",
        "\r\n",
        "                # Computing gradients and backpropagate in order to train the discriminator.\r\n",
        "                loss_d.backward()\r\n",
        "                optimizer_d.step()\r\n",
        "\r\n",
        "\r\n",
        "                ################################################################################################\r\n",
        "                #######################                                         ################################\r\n",
        "                #######################         GEN & ENC TRAINING              ################################\r\n",
        "                #######################                                         ################################\r\n",
        "                ################################################################################################\r\n",
        "\r\n",
        "\r\n",
        "                # Cleaning gradient.\r\n",
        "                optimizer_ge.zero_grad()\r\n",
        "\r\n",
        "                # Generator:\r\n",
        "                z_fake = Variable(torch.randn((x.size(0), self.args.latent_dim, 1, 1)).to(self.device),\r\n",
        "                                  requires_grad=False)\r\n",
        "                # compute G(z)\r\n",
        "                x_fake = self.G(z_fake)\r\n",
        "\r\n",
        "                # Encoder:\r\n",
        "                x_true = x.float().to(self.device)\r\n",
        "                # compute E(x)\r\n",
        "                z_true = self.E(x_true)\r\n",
        "\r\n",
        "                # Discriminator\r\n",
        "                # compute D(x, E(x))\r\n",
        "                out_true = self.D(x_true , z_true)#.squeeze(1).squeeze(1)\r\n",
        "                # compute D(G(z),z)\r\n",
        "                out_fake = self.D(x_fake , z_fake)#.squeeze(1).squeeze(1)\r\n",
        "\r\n",
        "                \r\n",
        "                \r\n",
        "                # compute losses\r\n",
        "                loss_ge = EG_loss(out_fake,out_true)\r\n",
        "                ge_loss_accuracy += loss_ge.item()\r\n",
        "\r\n",
        "                # Computing gradients and backpropagate in order to train the generator and encoder.\r\n",
        "                loss_ge.backward()\r\n",
        "                optimizer_ge.step()\r\n",
        "               \r\n",
        "  \r\n",
        "            stored_d_loss.append(d_loss_accuracy/len(self.train_loader))\r\n",
        "            stored_ge_loss.append(ge_loss_accuracy/len(self.train_loader))\r\n",
        "\r\n",
        "            if epoch % 1 == 0:\r\n",
        "                with torch.no_grad():\r\n",
        "                  z_fake = Variable(torch.randn((x.size(0), self.args.latent_dim, 1, 1)).to(self.device),\r\n",
        "                                    requires_grad=False)\r\n",
        "                  g_result = self.G(z_fake)\r\n",
        "                  ge_result = self.G(self.E(x.float().to(self.device)))\r\n",
        "\r\n",
        "                  vutils.save_image(g_result[:16].data, '/prj_gen_images/{}_G_fake.png'.format(epoch))\r\n",
        "                  vutils.save_image(x[:16].cpu().data, '/prj_gen_images/{}_X_fake.png'.format(epoch))\r\n",
        "                  vutils.save_image(ge_result[:16].data, '/prj_gen_images/{}_G(E(x))_fake.png'.format(epoch))\r\n",
        "\r\n",
        "                  g_img = mpimg.imread('/prj_gen_images/{}_G_fake.png'.format(epoch))\r\n",
        "                  ge_img = mpimg.imread('/prj_gen_images/{}_G(E(x))_fake.png'.format(epoch))\r\n",
        "                  x_img = mpimg.imread('/prj_gen_images/{}_X_fake.png'.format(epoch))\r\n",
        "\r\n",
        "\r\n",
        "                  fig, ax = plt.subplots(4, 1, figsize=(15, 7))\r\n",
        "                  fig.subplots_adjust(wspace=0.05, hspace=0.4)\r\n",
        "                  plt.rcParams.update({'font.size': 20})\r\n",
        "                  fig.suptitle('Epoch {}'.format(epoch))\r\n",
        "                  fig.text(0.04, 0.80, 'G(z)', ha='left')\r\n",
        "                  fig.text(0.04, 0.60, 'x', ha='left')\r\n",
        "                  fig.text(0.04, 0.40, 'G(E(x))', ha='left')\r\n",
        "                  # fig.text(0.04, 0.20, 'G(E(x))', ha='left')\r\n",
        "\r\n",
        "                  ax[0].imshow(g_img, cmap='gray')\r\n",
        "                  ax[0].axis('off')\r\n",
        "                  ax[1].imshow(x_img, cmap='gray')\r\n",
        "                  ax[1].axis('off')\r\n",
        "                  ax[2].imshow(ge_img, cmap='gray')\r\n",
        "                  ax[2].axis('off')\r\n",
        "                  # ax[3].set_xlim([0,i+1])\r\n",
        "                  ax[3].plot(stored_ge_loss,label='GE loss')\r\n",
        "                  ax[3].plot(stored_d_loss,label='D loss')\r\n",
        "                  ax[3].legend()\r\n",
        "                  plt.show()\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "            print(\"\\nTraining... Epoch: {}, Discrimiantor Loss: {:.3f}, Generator Loss: {:.3f}\".format(\r\n",
        "                epoch, d_loss_accuracy/i, ge_loss_accuracy/i)\r\n",
        "            )\r\n",
        "             \r\n",
        "\r\n",
        "        \r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1V2NLzYYin4"
      },
      "source": [
        "import argparse \r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "if __name__ == '__main__':\r\n",
        "    parser = argparse.ArgumentParser()\r\n",
        "    parser.add_argument(\"--num_epochs\", type=int, default=200,\r\n",
        "                        help=\"number of epochs\")\r\n",
        "    parser.add_argument('--lr_adam', type=float, default=1e-4,\r\n",
        "                        help='learning rate')\r\n",
        "    parser.add_argument('--lr_rmsprop', type=float, default=1e-4,\r\n",
        "                        help='learning rate RMSprop if WGAN is True.')\r\n",
        "    parser.add_argument(\"--batch_size\", type=int, default=128,\r\n",
        "                        help=\"Batch size\")\r\n",
        "    parser.add_argument('--latent_dim', type=int, default=256,\r\n",
        "                        help='Dimension of the latent variable z')\r\n",
        "    parser.add_argument('--wasserstein', type=bool, default=False,\r\n",
        "                        help='If WGAN.')\r\n",
        "    parser.add_argument('--clamp', type=float, default=1e-2,\r\n",
        "                        help='Clipping gradients for WGAN.')\r\n",
        "\r\n",
        "    #parsing arguments.\r\n",
        "    args = parser.parse_args(args=[]) \r\n",
        "\r\n",
        "    #check if cuda is available.\r\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n",
        "\r\n",
        "    train,test = get_cifar10(args)\r\n",
        "    # train,test = get_SVHN(args)\r\n",
        "    # train,test = get_CELEB_A(args)\r\n",
        "   \r\n",
        "    bigan = TrainerBiGAN(args, train, device)\r\n",
        "    bigan.train()\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iAb8qyFM5SlF"
      },
      "source": [
        "from google.colab import files\r\n",
        "import shutil\r\n",
        "\r\n",
        "shutil.make_archive('/prj_gen_images', 'zip', '/prj_gen_images')\r\n",
        "files.download('/prj_gen_images.zip') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwaGYQtNIFjg"
      },
      "source": [
        "model_D = 'D.pt'\r\n",
        "model_G = 'G.pt'\r\n",
        "model_E = 'E.pt'\r\n",
        "\r\n",
        "G,D,E = bigan.getModel()\r\n",
        "\r\n",
        "if not os.path.exists(\"/prj_models\"):\r\n",
        "    os.makedirs(\"/prj_models\")\r\n",
        "\r\n",
        "path = F\"/prj_models/{model_D}\" \r\n",
        "torch.save(D.state_dict(), path)\r\n",
        "path = F\"/prj_models/{model_G}\" \r\n",
        "torch.save(G.state_dict(), path)\r\n",
        "path = F\"/prj_models/{model_E}\" \r\n",
        "torch.save(E.state_dict(), path)\r\n",
        "\r\n",
        "shutil.make_archive('/prj_models', 'zip', '/prj_models')\r\n",
        "files.download('/prj_models.zip') \r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RadBG7vmaPIc"
      },
      "source": [
        "#1-NEAREST NEIGHBOUR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydjhlwAHaVK-"
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\r\n",
        "\r\n",
        "vedere perche non va\r\n",
        "\r\n",
        "print(DATASET)\r\n",
        "\r\n",
        "if DATASET == \"SVHN\":\r\n",
        "  print(\"SVHN\")\r\n",
        "  X_train, y_train = train.dataset.data, train.dataset.labels\r\n",
        "  X_test, y_test = test.dataset.data, test.dataset.labels\r\n",
        "elif DATASET == \"cifar10\":\r\n",
        "  print(\"cifar10\")\r\n",
        "  X_train, y_train = train.dataset.data, train.dataset.targets\r\n",
        "  X_test, y_test = test.dataset.data, test.dataset.targets\r\n",
        "\r\n",
        "E.eval()\r\n",
        "\r\n",
        "with torch.no_grad():\r\n",
        "    # EX_train = bigan.E(torch.tensor(X_train).float().to(bigan.device))\r\n",
        "    EX_train = bigan.E(torch.tensor(X_train).float())\r\n",
        "    EX_test = bigan.E(X_test)\r\n",
        "\r\n",
        "KNN = KNeighborsClassifier(n_neighbors=1)\r\n",
        "KNN.fit(EX_train, y_train)\r\n",
        "predictions = KNN.predict(EX_test)\r\n",
        "accuracy = np.sum(predictions == y_test) / len(y_test)\r\n",
        "print('Accuracy {:.2f}%'.format(accuracy*100))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}